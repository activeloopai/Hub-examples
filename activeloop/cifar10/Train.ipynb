{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4E4SxlOgWwB7"
   },
   "source": [
    "## Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Brl6r7zWwCA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install hub\n",
    "!pip install matplotlib\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWCVt8xYW7S_"
   },
   "source": [
    "## Log in to hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ZVxJWx2W6qx"
   },
   "outputs": [],
   "source": [
    "!hub login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jqWTq__WwCB"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FI0h7wcbWwCB"
   },
   "outputs": [],
   "source": [
    "import psutil\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import ConcatDataset\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "import hub\n",
    "from hub.compute.generic.ds_transforms import shift_scale_rotate, horizontal_flip\n",
    "from hub.api.sharded_datasetview import ShardedDatasetView"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjqmm-1kWwCC"
   },
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KDBVUTy5WwCC"
   },
   "outputs": [],
   "source": [
    "ds = hub.load(\"activeloop/cifar10_train\")\n",
    "ds_test = hub.load(\"activeloop/cifar10_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwZq_uNaWwCC"
   },
   "source": [
    "## Augment images and add to the original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-T47EdWWwCC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_augmented = horizontal_flip(shift_scale_rotate(ds, keys=['image'], rotate_limit=0, shift_limit=0.1), keys=['image'], p=0.2)\n",
    "ds_augmented = ds_augmented.store(\"/tmp/cidar10_aug\")\n",
    "ds_sharded = ShardedDatasetView([ds, ds_augmented])\n",
    "\n",
    "@hub.transform(schema=ds_sharded.schema, scheduler=\"threaded\", workers=psutil.cpu_count() - 1)\n",
    "def transform_identity(sample):\n",
    "    return sample\n",
    "\n",
    "ds = transform_identity(ds_sharded).store('/tmp/cifar10_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzgztA2iWwCD"
   },
   "source": [
    "## Define a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyIIdZShWwCE"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.reshape(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqoHIZ4CWwCE"
   },
   "source": [
    "## Training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hM67zZP1WwCE"
   },
   "outputs": [],
   "source": [
    "def train(trainloader: torch.utils.data.DataLoader, valloader: torch.utils.data.DataLoader, net: nn.Module):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)\n",
    "    for epoch in range(10):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            X, y = data\n",
    "            X = X.permute(0, 3, 1, 2).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(X)\n",
    "            loss = criterion(outputs, y.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if not i % 100:\n",
    "                print(f\"Loss {loss.item()}\")\n",
    "        validate(net, valloader)\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Yq2FcBIWwCF"
   },
   "outputs": [],
   "source": [
    "def validate(net, valloader):\n",
    "    correct_count, all_count = 0, 0\n",
    "    for i, data in enumerate(valloader):\n",
    "        X, y = data\n",
    "        if len(X.shape) != 4:\n",
    "            X = torch.unsqueeze(X, 0)\n",
    "        X = X.permute(0, 3, 1, 2).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = net(X)\n",
    "        pred_label = outputs.argmax(1)\n",
    "        correct_count += np.sum(pred_label.numpy() == y.numpy())\n",
    "        all_count += len(pred_label)\n",
    "\n",
    "    print(\"Number Of Images Tested =\", all_count)\n",
    "    print(\"Model Accuracy =\", (correct_count/all_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlmD12X7WwCF"
   },
   "source": [
    "## Convert to PyTorch, split the data and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FS3RhSFeWwCF"
   },
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    img = data['image'].float()\n",
    "    label = data['label']\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3BRunB8WwCF"
   },
   "outputs": [],
   "source": [
    "torch_ds = ds.to_pytorch(transform=transform)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = Net()\n",
    "net = net.to(device)\n",
    "train_len = int(0.8 * len(torch_ds))\n",
    "test_len = len(torch_ds) - train_len\n",
    "train_ds, val_ds = random_split(torch_ds, [train_len, test_len])\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=8,\n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=8,\n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "train(train_dataloader, val_dataloader, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HMAOWHmWwCG"
   },
   "outputs": [],
   "source": [
    "torch_ds_test = ds_test.to_pytorch(transform=transform)\n",
    "validate(net, torch_ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HDYX689WWwCG"
   },
   "outputs": [],
   "source": [
    "torch.save(net, \"/tmp/cifar10_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('venv': venv)",
   "language": "python",
   "name": "python36964bitvenvvenv2f96fd8b646d44e3b9e0885369a63a85"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
